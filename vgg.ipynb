{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the image paths\n",
    "image_paths = list(paths.list_images('input/Images'))\n",
    "# create an empty DataFrame\n",
    "data = pd.DataFrame()\n",
    "labels = []\n",
    "for i, image_path in tqdm(enumerate(image_paths), total=len(image_paths)):\n",
    "    label = image_path.split(os.path.sep)[-2]\n",
    "    data.loc[i, 'image_path'] = image_path\n",
    "    labels.append(label)\n",
    "    \n",
    "labels = np.array(labels)\n",
    "# one hot encode\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "\n",
    "print(f\"The first one hot encoded labels: {labels[0]}\")\n",
    "print(f\"Mapping an one hot encoded label to its category: {lb.classes_[0]}\")\n",
    "print(f\"Total instances: {len(labels)}\")\n",
    "for i in range(len(labels)):\n",
    "    index = np.argmax(labels[i])\n",
    "    #index = labels[i]\n",
    "\n",
    "    data.loc[i, 'target'] = int(index)\n",
    "# shuffle the dataset\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "# save as csv file\n",
    "data.to_csv('input/data.csv', index=False)\n",
    "# pickle the label binarizer\n",
    "joblib.dump(lb, 'output/lb.pkl')\n",
    "print('Save the one-hot encoded binarized labels as a pickled file.')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import albumentations\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(SEED=42):\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.benchmark = True \n",
    "SEED=42\n",
    "seed_everything(SEED=SEED)\n",
    "''' SEED Everything '''\n",
    "# set computation device\n",
    "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(pretrained, requires_grad):\n",
    "    model = models.vgg16(progress=True, pretrained=pretrained)\n",
    "\n",
    "    \n",
    "    \n",
    "    # freeze hidden layers\n",
    "    if requires_grad == False:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    # train the hidden layers\n",
    "    elif requires_grad == True:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    # make the classification layer learnable\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(1000, len(lb.classes_))\n",
    "    )\n",
    "    return model\n",
    "model = model(pretrained=True, requires_grad=True).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, weight_decay=0.0005)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning rate=1e-4, momentum=0.9, weight decay=0.0005, \n",
    "batch size = 16, epoch = 50 \n",
    "criterion = Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation function\n",
    "def validate(model, test_dataloader):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(test_dataloader), total=int(len(test_data)/test_dataloader.batch_size)):\n",
    "            data, target = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            val_running_correct += (preds == target).sum().item()\n",
    "        \n",
    "        val_loss = val_running_loss/len(test_dataloader.dataset)\n",
    "        val_accuracy = 100. * val_running_correct/len(test_dataloader.dataset)\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}')\n",
    "        \n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def fit(model, train_dataloader):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    for i, data in tqdm(enumerate(train_dataloader), total=int(len(train_data)/train_dataloader.batch_size)):\n",
    "        data, target = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_running_loss += loss.item()\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == target).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = train_running_loss/len(train_dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(train_dataloader.dataset)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}\")\n",
    "    \n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss , train_accuracy = [], []\n",
    "val_loss , val_accuracy = [], []\n",
    "start = time.time()\n",
    "for epoch in range(args['epochs']):\n",
    "    print(f\"Epoch {epoch+1} of {args['epochs']}\")\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model, trainloader)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model, testloader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_accuracy.append(train_epoch_accuracy)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)\n",
    "end = time.time()\n",
    "print(f\"{(end-start)/60:.3f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_accuracy, color='green', label='train accuracy')\n",
    "plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig('accuracy_vgg_dropout.jpg')\n",
    "plt.show()\n",
    "# loss plots\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_loss, color='orange', label='train loss')\n",
    "plt.plot(val_loss, color='red', label='validataion loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('loss_vgg_dropout.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(11))\n",
    "class_total = list(0. for i in range(11))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(labels.shape[0]):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "print(class_correct)\n",
    "print(class_total)\n",
    "print(sum(class_correct)/sum(class_total))\n",
    "for i in range(11):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        vows[i], 100 * class_correct[i] / class_total[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bae2fed8b218de23cfc7a580f0c20f861598714f26c0821e60213b94b4c1834"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
